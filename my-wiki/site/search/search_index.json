{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>\u81ea\u6d4b\u547d\u4ee4 mkdocs serve   mkdocs gh-deploy\nmkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre> <p>wiki\u8d44\u6e90\u7f51\u7ad9 https://lianghaoxun.github.io/pytorch_for_l_learn/ \u7b2c 1 \u6b65\uff1a\u751f\u6210 SSH \u5bc6\u94a5 ssh-keygen git init git commit -m\"update\" git remote add origin git@github.com:lianghaoxun/pytorch_for_l_learn.git git push -u origin main</p>"},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/","title":"\u4e00\u3001pyTorch \u5de5\u4f5c\u6d41\u7a0b","text":"<p>\u5185\u5bb9</p> <p>1. \u51c6\u5907\u6570\u636e\u5904\u7406\u597d\u6570\u636e\u5fc5\u987b\u662f\u5f20\u91cf\u683c\u5f0f\u6765\u8fdb\u884c\u8bad\u7ec3</p> <p>2. \u5efa\u7acb\u6a21\u578b\u521b\u5efa\u4e00\u4e2a\u6a21\u578b\u6765\u5b66\u4e60\u6570\u636e\u4e2d\u7684\u6a21\u5f0f\uff0c\u9009\u62e9\u635f\u5931\u51fd\u6570\u3001\u4f18\u5316\u5668\u5e76\u6784\u5efa\u8bad\u7ec3\u5faa\u73af\u3002</p> <p>3. \u5c06\u6a21\u578b\u62df\u5408\u5230\u6570\u636e\uff08\u8bad\u7ec3\uff09\u6211\u4eec\u5df2\u7ecf\u6709\u4e86\u6570\u636e\u548c\u6a21\u578b\uff0c\u73b0\u5728\u8ba9\u6211\u4eec\u8ba9\u6a21\u578b\uff08\u5c1d\u8bd5\uff09\u5728\uff08\u8bad\u7ec3\uff09\u6570\u636e\u4e2d\u67e5\u627e\u6a21\u5f0f\u3002</p> <p>4. \u8fdb\u884c\u9884\u6d4b\u5e76\u8bc4\u4f30\u6a21\u578b\uff08\u63a8\u7406\uff09\u6211\u4eec\u7684\u6a21\u578b\u5728\u6570\u636e\u4e2d\u53d1\u73b0\u4e86\u6a21\u5f0f\uff0c\u8ba9\u6211\u4eec\u5c06\u5176\u53d1\u73b0\u4e0e\u5b9e\u9645\uff08\u6d4b\u8bd5\uff09\u6570\u636e\u8fdb\u884c\u6bd4\u8f83\u3002</p> <p>5. \u4fdd\u5b58\u548c\u52a0\u8f7d\u6a21\u578b\u60a8\u53ef\u80fd\u60f3\u5728\u5176\u4ed6\u5730\u65b9\u4f7f\u7528\u60a8\u7684\u6a21\u578b\uff0c\u6216\u8005\u7a0d\u540e\u518d\u56de\u6765\u4f7f\u7528\u5b83\uff0c\u6211\u4eec\u5c06\u5728\u6b64\u5904\u4ecb\u7ecd\u8fd9\u4e00\u70b9\u3002</p> <p>6. \u5c06\u6240\u6709\u5185\u5bb9\u653e\u5728\u4e00\u8d77\u8ba9\u6211\u4eec\u5c06\u4ee5\u4e0a\u6240\u6709\u5185\u5bb9\u7ed3\u5408\u8d77\u6765\u3002</p> <p></p>"},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#_1","title":"\u4e8c\u3001\u5efa\u7acb\u6a21\u578b","text":""},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#1-pytorch","title":"1. PyTorch \u6a21\u578b\u6784\u5efa\u8981\u70b9","text":"<p>PyTorch \u6709\u56db\u4e2a\uff08\u7ed9\u4e88\u6216\u63a5\u53d7\uff09\u57fa\u672c\u6a21\u5757\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u5b83\u6765\u521b\u5efa\u60a8\u53ef\u4ee5\u60f3\u8c61\u7684\u51e0\u4e4e\u4efb\u4f55\u7c7b\u578b\u7684\u795e\u7ecf\u7f51\u7edc\u3002</p> <p>\u5b83\u4eec\u662f<code>torch.nn</code>\u3001<code>torch.optim</code>\u3001<code>torch.utils.data.Dataset</code>\u548c<code>torch.utils.data.DataLoader</code></p> <p>\u4ee5\u4e0b\u901a\u8fc7<code>torch.nn</code>\u6784\u5efa\u7b80\u5355\u7684\u6a21\u578b</p> <ul> <li><code>nn.Module</code>\u5305\u542b\u8f83\u5927\u7684\u6784\u5efa\u5757\uff08\u5c42\uff09</li> <li><code>nn.Parameter</code>\u5305\u542b\u8f83\u5c0f\u7684\u53c2\u6570\uff0c\u4f8b\u5982\u6743\u91cd\u548c\u504f\u5dee\uff08\u5c06\u5b83\u4eec\u653e\u5728\u4e00\u8d77\u6784\u6210<code>nn.Module</code>\uff08s\uff09\uff09</li> <li><code>forward()</code>\u544a\u8bc9\u8f83\u5927\u7684\u5757\u5982\u4f55\u5728 <code>nn.Module</code>\uff08s\uff09\u5185\u5bf9\u8f93\u5165\uff08\u5145\u6ee1\u6570\u636e\u7684\u5f20\u91cf\uff09\u8fdb\u884c\u8ba1\u7b97</li> <li><code>torch.optim</code>\u5305\u542b\u5982\u4f55\u6539\u8fdb\u53c2\u6570\u4ee5<code>nn.Parameter</code>\u66f4\u597d\u5730\u8868\u793a\u8f93\u5165\u6570\u636e\u7684\u4f18\u5316\u65b9\u6cd5</li> </ul>"},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#2y-mx-b","title":"2.\u7ebf\u6027\u56de\u5f52\u6a21\u578b(y = m*x + b)","text":"<pre><code># Create a Linear Regression model class\nclass LinearRegressionModel(nn.Module): # &lt;- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)\n    def __init__(self):\n        super().__init__() \n        self.weights = nn.Parameter(torch.randn(1, # &lt;- start with random weights (this will get adjusted as the model learns)\n                                                dtype=torch.float), # &lt;- PyTorch loves float32 by default\n                                   requires_grad=True) # &lt;- can we update this value with gradient descent?)\n\n        self.bias = nn.Parameter(torch.randn(1, # &lt;- start with random bias (this will get adjusted as the model learns)\n                                            dtype=torch.float), # &lt;- PyTorch loves float32 by default\n                                requires_grad=True) # &lt;- can we update this value with gradient descent?))\n\n    # Forward defines the computation in the model\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor: # &lt;- \"x\" is the input data (e.g. training/testing features)\n        return self.weights * x + self.bias # &lt;- this is the linear regression formula (y = m*x + b)\n</code></pre>"},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#1","title":"\uff081\uff09.\u8f93\u51fa\u6a21\u578b\u7684\u72b6\u6001","text":"<pre><code># Set manual seed since nn.Parameter are randomly initialzied\ntorch.manual_seed(42)\n\n# Create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s))\nmodel_0 = LinearRegressionModel()\n\n# Check the nn.Parameter(s) within the nn.Module subclass we created\nlist(model_0.parameters())\n</code></pre> <pre><code># List named parameters \nmodel_0.state_dict()\n</code></pre>"},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#2","title":"\uff082\uff09.\u6784\u5efa\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668","text":"<pre><code># Create the loss function\nloss_fn = nn.L1Loss() # MAE loss is same as L1Loss\n\n# Create the optimizer\noptimizer = torch.optim.SGD(params=model_0.parameters(), # parameters of target model to optimize\n                            lr=0.01) # learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time))\n\n</code></pre>"},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#3","title":"\uff083\uff09.\u6a21\u578b\u9884\u6d4b","text":"<pre><code># 1. Set the model in evaluation mode\nmodel_0.eval()\n\n# 2. Setup the inference mode context manager\nwith torch.inference_mode():\n  # 3. Make sure the calculations are done with the model and data on the same device\n  # in our case, we haven't setup device-agnostic code yet so our data and model are\n  # on the CPU by default.\n  # model_0.to(device)\n  # X_test = X_test.to(device)\n  y_preds = model_0(X_test)\ny_preds\n</code></pre>"},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#_2","title":"\u4e09\u3001\u8bad\u7ec3\u6a21\u578b","text":""},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#1_1","title":"1.\u8bad\u7ec3\u8fc7\u7a0b","text":"\u6570\u5b57 \u6b65\u9aa4\u540d\u79f0 \u5b83\u6709\u4ec0\u4e48\u4f5c\u7528\uff1f \u4ee3\u7801\u793a\u4f8b 1 \u524d\u4f20 \u8be5\u6a21\u578b\u4e00\u6b21\u904d\u5386\u6240\u6709\u8bad\u7ec3\u6570\u636e\uff0c\u6267\u884c\u5176<code>forward()</code>\u51fd\u6570\u8ba1\u7b97\u3002 <code>model(x_train)</code> 2 \u8ba1\u7b97\u635f\u5931 \u5c06\u6a21\u578b\u7684\u8f93\u51fa\uff08\u9884\u6d4b\uff09\u4e0e\u771f\u5b9e\u60c5\u51b5\u8fdb\u884c\u6bd4\u8f83\u5e76\u8fdb\u884c\u8bc4\u4f30\uff0c\u4ee5\u4e86\u89e3\u5b83\u4eec\u7684\u9519\u8bef\u7a0b\u5ea6\u3002 <code>loss = loss_fn(y_pred, y_train)</code> 3 \u96f6\u68af\u5ea6 \u4f18\u5316\u5668\u68af\u5ea6\u8bbe\u7f6e\u4e3a\u96f6\uff08\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u4f1a\u7d2f\u79ef\uff09\uff0c\u56e0\u6b64\u53ef\u4ee5\u9488\u5bf9\u7279\u5b9a\u8bad\u7ec3\u6b65\u9aa4\u91cd\u65b0\u8ba1\u7b97\u5b83\u4eec\u3002 <code>optimizer.zero_grad()</code> 4 \u5bf9\u635f\u5931\u6267\u884c\u53cd\u5411\u4f20\u64ad \u8ba1\u7b97\u6bcf\u4e2a\u8981\u66f4\u65b0\u7684\u6a21\u578b\u53c2\u6570\u7684\u635f\u5931\u68af\u5ea6\uff08\u6bcf\u4e2a\u53c2\u6570\u5e26\u6709<code>requires_grad=True</code>\uff09\u3002\u8fd9\u79f0\u4e3a\u53cd\u5411\u4f20\u64ad\uff0c\u56e0\u6b64\u662f\u201c\u5411\u540e\u201d\u3002 <code>loss.backward()</code> 5 \u66f4\u65b0\u4f18\u5316\u5668\uff08\u68af\u5ea6\u4e0b\u964d\uff09 \u66f4\u65b0\u5173\u4e8e\u635f\u5931\u68af\u5ea6\u7684\u53c2\u6570<code>requires_grad=True</code>\u4ee5\u6539\u8fdb\u5b83\u4eec\u3002 <code>optimizer.step()</code>"},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#2_1","title":"2.\u6d4b\u8bd5\u8fc7\u7a0b","text":"\u6570\u5b57 \u6b65\u9aa4\u540d\u79f0 \u5b83\u6709\u4ec0\u4e48\u4f5c\u7528\uff1f \u4ee3\u7801\u793a\u4f8b 1 \u524d\u4f20 \u8be5\u6a21\u578b\u4e00\u6b21\u904d\u5386\u6240\u6709\u8bad\u7ec3\u6570\u636e\uff0c\u6267\u884c\u5176<code>forward()</code>\u51fd\u6570\u8ba1\u7b97\u3002 <code>model(x_test)</code> 2 \u8ba1\u7b97\u635f\u5931 \u5c06\u6a21\u578b\u7684\u8f93\u51fa\uff08\u9884\u6d4b\uff09\u4e0e\u771f\u5b9e\u60c5\u51b5\u8fdb\u884c\u6bd4\u8f83\u5e76\u8fdb\u884c\u8bc4\u4f30\uff0c\u4ee5\u4e86\u89e3\u5b83\u4eec\u7684\u9519\u8bef\u7a0b\u5ea6\u3002 <code>loss = loss_fn(y_pred, y_test)</code> 3 \u8ba1\u7b97\u8bc4\u4f30\u6307\u6807\uff08\u53ef\u9009\uff09 \u9664\u4e86\u635f\u5931\u503c\u4e4b\u5916\uff0c\u60a8\u53ef\u80fd\u8fd8\u9700\u8981\u8ba1\u7b97\u5176\u4ed6\u8bc4\u4f30\u6307\u6807\uff0c\u4f8b\u5982\u6d4b\u8bd5\u96c6\u7684\u51c6\u786e\u6027\u3002 \u81ea\u5b9a\u4e49\u529f\u80fd"},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#3_1","title":"3.\u7ebf\u6027\u56de\u5f52\u6848\u4f8b\u8bad\u7ec3\u5b8c\u6574\u8fc7\u7a0b","text":"<pre><code>torch.manual_seed(42)\n\n# Set the number of epochs (how many times the model will pass over the training data)\nepochs = 100\n\n# Create empty loss lists to track values\ntrain_loss_values = []\ntest_loss_values = []\nepoch_count = []\n\nfor epoch in range(epochs):\n    ### Training\n\n    # Put model in training mode (this is the default state of a model)\n    model_0.train()\n\n    # 1. Forward pass on train data using the forward() method inside \n    y_pred = model_0(X_train)\n    # print(y_pred)\n\n    # 2. Calculate the loss (how different are our models predictions to the ground truth)\n    loss = loss_fn(y_pred, y_train)\n\n    # 3. Zero grad of the optimizer\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Progress the optimizer\n    optimizer.step()\n\n    ### Testing\n\n    # Put the model in evaluation mode\n    model_0.eval()\n\n    with torch.inference_mode():\n      # 1. Forward pass on test data\n      test_pred = model_0(X_test)\n\n      # 2. Caculate loss on test data\n      test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type\n\n      # Print out what's happening\n      if epoch % 10 == 0:\n            epoch_count.append(epoch)\n            train_loss_values.append(loss.detach().numpy())\n            test_loss_values.append(test_loss.detach().numpy())\n            print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")\n\n</code></pre>"},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#1_2","title":"\uff081\uff09\u7ed8\u5236\u8bad\u7ec3\u6d4b\u8bd5\u7684\u635f\u5931\u51fd\u6570\u66f2\u7ebf","text":"<pre><code># Plot the loss curves\nplt.plot(epoch_count, train_loss_values, label=\"Train loss\")\nplt.plot(epoch_count, test_loss_values, label=\"Test loss\")\nplt.title(\"Training and test loss curves\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend();\n</code></pre>"},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#_3","title":"\u56db\u3001\u52a0\u8f7d\u4fdd\u5b58\u6a21\u578b","text":""},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#1_3","title":"1.\u52a0\u8f7d\u6a21\u578b","text":"<pre><code>from pathlib import Path\n\n# 1. Create models directory \nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, exist_ok=True)\n\n# 2. Create model save path \nMODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# 3. Save the model state dict \nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_0.state_dict(), # only saving the state_dict() only saves the models learned parameters\n           f=MODEL_SAVE_PATH) \n</code></pre>"},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#2_2","title":"2.\u4fdd\u5b58\u6a21\u578b","text":"<pre><code># Instantiate a new instance of our model (this will be instantiated with random weights)\nloaded_model_0 = LinearRegressionModel()\n\n# Load the state_dict of our saved model (this will update the new instance of our model with trained weights)\nloaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n</code></pre>"},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#_4","title":"\u4e94\u3001\u6240\u6709\u5185\u5bb9\u6574\u5408","text":""},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#1pytorch","title":"1.\u5bfc\u5165\u3001pytorch\u3001\u8c03\u7528\u8d44\u6e90","text":"<pre><code># Import PyTorch and matplotlib\nimport torch\nfrom torch import nn # nn contains all of PyTorch's building blocks for neural networks\nimport matplotlib.pyplot as plt\n\n# Check PyTorch version\ntorch.__version__\n</code></pre> <pre><code># Setup device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n</code></pre>"},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#2model_1","title":"2.\u6a21\u578b\u5f15\u7528\u8d44\u6e90model_1\u4e3a\u4f8b\u5b50","text":"<pre><code># Set model to GPU if it's availalble, otherwise it'll default to CPU\nmodel_1.to(device) # the device variable was set above to be \"cuda\" if available or \"cpu\" if not\nnext(model_1.parameters()).device\n</code></pre>"},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#2_3","title":"2.\u6570\u636e\u96c6","text":""},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#1_4","title":"\uff081\uff09\u7ebf\u6027\u56de\u5f52\u6848\u4f8b","text":"<pre><code># Create weight and bias\nweight = 0.7\nbias = 0.3\n\n# Create range values\nstart = 0\nend = 1\nstep = 0.02\n\n# Create X and y (features and labels)\nX = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will happen later on (shapes within linear layers)\ny = weight * X + bias \n# Split data\ntrain_split = int(0.8 * len(X))\nX_train, y_train = X[:train_split], y[:train_split]\nX_test, y_test = X[train_split:], y[train_split:]\n\nlen(X_train), len(y_train), len(X_test), len(y_test)\n</code></pre>"},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#3_2","title":"3.\u5efa\u7acb\u6a21\u578b","text":""},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#1_5","title":"\uff081\uff09\u6784\u5efa\u635f\u5931\u4f18\u5316\u5668","text":"<pre><code># Create loss function\nloss_fn = nn.L1Loss()\n\n# Create optimizer\noptimizer = torch.optim.SGD(params=model_1.parameters(), # optimize newly created model's parameters\n                            lr=0.01)\n</code></pre>"},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#2_4","title":"\uff082\uff09\u7ebf\u6027\u56de\u5f52\u6848\u4f8b","text":"<pre><code># Subclass nn.Module to make our model\nclass LinearRegressionModelV2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Use nn.Linear() for creating the model parameters\n        self.linear_layer = nn.Linear(in_features=1, \n                                      out_features=1)\n\n    # Define the forward computation (input data x flows through nn.Linear())\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.linear_layer(x)\n\n# Set the manual seed when creating the model (this isn't always need but is used for demonstrative purposes, try commenting it out and seeing what happens)\ntorch.manual_seed(42)\nmodel_1 = LinearRegressionModelV2()\nmodel_1, model_1.state_dict()\n</code></pre>"},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#4","title":"4.\u8bad\u7ec3\u6a21\u578b","text":""},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#1_6","title":"\uff081\uff09\u7ebf\u6027\u56de\u5f52\u6848\u4f8b","text":"<pre><code>torch.manual_seed(42)\n\n# Set the number of epochs \nepochs = 1000 \n\n# Put data on the available device\n# Without this, error will happen (not all model/data on device)\nX_train = X_train.to(device)\nX_test = X_test.to(device)\ny_train = y_train.to(device)\ny_test = y_test.to(device)\n\nfor epoch in range(epochs):\n    ### Training\n    model_1.train() # train mode is on by default after construction\n\n    # 1. Forward pass\n    y_pred = model_1(X_train)\n\n    # 2. Calculate loss\n    loss = loss_fn(y_pred, y_train)\n\n    # 3. Zero grad optimizer\n    optimizer.zero_grad()\n\n    # 4. Loss backward\n    loss.backward()\n\n    # 5. Step the optimizer\n    optimizer.step()\n\n    ### Testing\n    model_1.eval() # put the model in evaluation mode for testing (inference)\n    # 1. Forward pass\n    with torch.inference_mode():\n        test_pred = model_1(X_test)\n\n        # 2. Calculate the loss\n        test_loss = loss_fn(test_pred, y_test)\n\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")\n</code></pre>"},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#5","title":"5.\u8bc4\u4f30\u6a21\u578b","text":""},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#1_7","title":"\uff081\uff09\u7ebf\u6027\u56de\u5f52\u6848\u4f8b","text":"<p>\u8f93\u51fa\u53c2\u6570</p> <pre><code># Find our model's learned parameters\nfrom pprint import pprint # pprint = pretty print, see: https://docs.python.org/3/library/pprint.html \nprint(\"The model learned the following values for weights and bias:\")\npprint(model_1.state_dict())\nprint(\"\\nAnd the original values for weights and bias are:\")\nprint(f\"weights: {weight}, bias: {bias}\")\n</code></pre> <p>\u9884\u6d4b</p> <pre><code># Turn model into evaluation mode\nmodel_1.eval()\n\n# Make predictions on the test data\nwith torch.inference_mode():\n    y_preds = model_1(X_test)\ny_preds\n</code></pre> <pre><code>\ndef plot_predictions(train_data=X_train, \n                     train_labels=y_train, \n                     test_data=X_test, \n                     test_labels=y_test, \n                     predictions=None):\n  \"\"\"\n  Plots training data, test data and compares predictions.\n  \"\"\"\n  plt.figure(figsize=(10, 7))\n\n  # Plot training data in blue\n  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n\n  # Plot test data in green\n  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n\n  if predictions is not None:\n    # Plot the predictions in red (predictions were made on the test data)\n    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n\n  # Show the legend\n  plt.legend(prop={\"size\": 14});\n# plot_predictions(predictions=y_preds) # -&gt; won't work... data not on CPU\n\n# Put data on the CPU and plot it\nplot_predictions(predictions=y_preds.cpu())\n</code></pre>"},{"location":"1.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90/#6","title":"6.\u4fdd\u5b58\u52a0\u8f7d\u6a21\u578b","text":"<p>\u4fdd\u5b58</p> <pre><code>from pathlib import Path\n\n# 1. Create models directory \nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, exist_ok=True)\n\n# 2. Create model save path \nMODEL_NAME = \"01_pytorch_workflow_model_1.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# 3. Save the model state dict \nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_1.state_dict(), # only saving the state_dict() only saves the models learned parameters\n           f=MODEL_SAVE_PATH) \n</code></pre> <p>\u52a0\u8f7d</p> <pre><code># Instantiate a fresh instance of LinearRegressionModelV2\nloaded_model_1 = LinearRegressionModelV2()\n\n# Load model state dict \nloaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))\n\n# Put model to target device (if your data is on GPU, model will have to be on GPU to make predictions)\nloaded_model_1.to(device)\n\nprint(f\"Loaded model:\\n{loaded_model_1}\")\nprint(f\"Model on device:\\n{next(loaded_model_1.parameters()).device}\")\n</code></pre>"},{"location":"2.%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BD%95/","title":"\u4e00\u3001\u6a21\u578b","text":""},{"location":"2.%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BD%95/#1","title":"1.\u4e8c\u5206\u7c7b\u6a21\u578b","text":"<p>\u8fd9\u91cc\u4ee5\u4e00\u4e2a\u4e24\u5217\u9884\u6d4b\u4e00\u5217\u4e3a\u4f8b\u5b50</p> <pre><code># 1. Construct a model class that subclasses nn.Module\nclass CircleModelV0(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # 2. Create 2 nn.Linear layers capable of handling X and y input and output shapes\n        self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features (X), produces 5 features\n        self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features, produces 1 feature (y)\n\n    # 3. Define a forward method containing the forward pass computation\n    def forward(self, x):\n        # Return the output of layer_2, a single feature, the same shape as y\n        return self.layer_2(self.layer_1(x)) # computation goes through layer_1 first then the output of layer_1 goes through layer_2\n\n# 4. Create an instance of the model and send it to target device\nmodel_0 = CircleModelV0().to(device)\nmodel_0\n</code></pre>"},{"location":"2.%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BD%95/#1_1","title":"\uff081\uff09\u4f18\u5316\u4f8b\u5982\u52a0\u5165\u591a\u4e2a\u5c42","text":"<pre><code>class CircleModelV1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n        self.layer_2 = nn.Linear(in_features=10, out_features=10) # extra layer\n        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n\n    def forward(self, x): # note: always make sure forward is spelt correctly!\n        # Creating a model like this is the same as below, though below\n        # generally benefits from speedups where possible.\n        # z = self.layer_1(x)\n        # z = self.layer_2(z)\n        # z = self.layer_3(z)\n        # return z\n        return self.layer_3(self.layer_2(self.layer_1(x)))\n\nmodel_1 = CircleModelV1().to(device)\nmodel_1\n</code></pre>"},{"location":"2.%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BD%95/#2","title":"\uff082\uff09\u8bad\u7ec3\u8fc7\u7a0b","text":"<pre><code>torch.manual_seed(42)\n# Create a loss function\n# loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in\nloss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid built-in\n\n# Create an optimizer\noptimizer = torch.optim.SGD(params=model_0.parameters(), \n                            lr=0.1)\nepochs = 1000 # Train for longer\n\n# Put data to target device\nX_train, y_train = X_train.to(device), y_train.to(device)\nX_test, y_test = X_test.to(device), y_test.to(device)\n\nfor epoch in range(epochs):\n    ### Training\n    # 1. Forward pass\n    y_logits = model_1(X_train).squeeze()\n    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -&gt; predicition probabilities -&gt; prediction labels\n\n    # 2. Calculate loss/accuracy\n    loss = loss_fn(y_logits, y_train)\n    acc = accuracy_fn(y_true=y_train, \n                      y_pred=y_pred)\n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_1.eval()\n    with torch.inference_mode():\n        # 1. Forward pass\n        test_logits = model_1(X_test).squeeze() \n        test_pred = torch.round(torch.sigmoid(test_logits))\n        # 2. Caculate loss/accuracy\n        test_loss = loss_fn(test_logits,\n                            y_test)\n        test_acc = accuracy_fn(y_true=y_test,\n                               y_pred=test_pred)\n\n    # Print out what's happening every 10 epochs\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n\n</code></pre>"},{"location":"2.%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BD%95/#3sequential","title":"\uff083\uff09Sequential\u53ef\u4ee5\u5feb\u901f\u751f\u6210","text":"<pre><code># Same architecture as model_1 (but using nn.Sequential)\nmodel_2 = nn.Sequential(\n    nn.Linear(in_features=1, out_features=10),\n    nn.Linear(in_features=10, out_features=10),\n    nn.Linear(in_features=10, out_features=1)\n).to(device)\n\nmodel_2\n</code></pre>"},{"location":"2.%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BD%95/#4relu","title":"\uff084\uff09\u4f18\u5316\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\u7684ReLU\u793a\u8303","text":"<p>PyTorch \u6709\u4e00\u5806\u73b0\u6210\u7684\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\uff0c\u5b83\u4eec\u53ef\u4ee5\u505a\u7c7b\u4f3c\u4f46\u4e0d\u540c\u7684\u4e8b\u60c5\u3002</p> <p>\u6700\u5e38\u89c1\u548c\u6027\u80fd\u6700\u597d\u7684\u4e4b\u4e00\u662f [ReLU]( https://en.wikipedia.org/wiki/Rectifier_(neural_networks ) \uff08\u4fee\u6b63\u7ebf\u6027\u5355\u5143\uff0c<code>torch.nn.ReLU()</code>\uff09\u3002</p> <pre><code># Build model with non-linear activation function\nfrom torch import nn\nclass CircleModelV2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n        self.layer_2 = nn.Linear(in_features=10, out_features=10)\n        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n        self.relu = nn.ReLU() # &lt;- add in ReLU activation function\n        # Can also put sigmoid in the model \n        # This would mean you don't need to use it on the predictions\n        # self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n      # Intersperse the ReLU activation function between layers\n       return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n\nmodel_3 = CircleModelV2().to(device)\nprint(model_3)\n</code></pre>"},{"location":"2.%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BD%95/#5","title":"\uff085\uff09\u9884\u6d4b","text":"<pre><code># Make predictions\nmodel_3.eval()\nwith torch.inference_mode():\n    y_preds = torch.round(torch.sigmoid(model_3(X_test))).squeeze()\ny_preds[:10], y[:10] # want preds in same format as truth labels\n</code></pre>"},{"location":"2.%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BD%95/#2_1","title":"2.\u591a\u5206\u7c7b","text":"<p>\u591a\u5206\u7c7b\u6a21\u578b</p> <pre><code>from torch import nn\n\n# Build model\nclass BlobModel(nn.Module):\n    def __init__(self, input_features, output_features, hidden_units=8):\n        \"\"\"Initializes all required hyperparameters for a multi-class classification model.\n\n        Args:\n            input_features (int): Number of input features to the model.\n            out_features (int): Number of output features of the model\n              (how many classes there are).\n            hidden_units (int): Number of hidden units between layers, default 8.\n        \"\"\"\n        super().__init__()\n        self.linear_layer_stack = nn.Sequential(\n            nn.Linear(in_features=input_features, out_features=hidden_units),\n            # nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n            # nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n            nn.Linear(in_features=hidden_units, out_features=output_features), # how many classes are there?\n        )\n\n    def forward(self, x):\n        return self.linear_layer_stack(x)\n\n# Create an instance of BlobModel and send it to the target device\nmodel_4 = BlobModel(input_features=NUM_FEATURES, \n                    output_features=NUM_CLASSES, \n                    hidden_units=8).to(device)\nmodel_4\n</code></pre>"},{"location":"2.%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BD%95/#1_2","title":"\uff081\uff09\u8bad\u7ec3","text":"<pre><code># Fit the model\ntorch.manual_seed(42)\n\n# Set number of epochs\nepochs = 100\n\n# Put data to target device\nX_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device)\nX_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)\n\nfor epoch in range(epochs):\n    ### Training\n    model_4.train()\n\n    # 1. Forward pass\n    y_logits = model_4(X_blob_train) # model outputs raw logits \n    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -&gt; prediction probabilities -&gt; prediction labels\n    # print(y_logits)\n    # 2. Calculate loss and accuracy\n    loss = loss_fn(y_logits, y_blob_train) \n    acc = accuracy_fn(y_true=y_blob_train,\n                      y_pred=y_pred)\n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_4.eval()\n    with torch.inference_mode():\n      # 1. Forward pass\n      test_logits = model_4(X_blob_test)\n      test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n      # 2. Calculate test loss and accuracy\n      test_loss = loss_fn(test_logits, y_blob_test)\n      test_acc = accuracy_fn(y_true=y_blob_test,\n                             y_pred=test_pred)\n\n    # Print out what's happening\n    if epoch % 10 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\") \n</code></pre>"},{"location":"2.%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BD%95/#_2","title":"\u4e8c\u3001\u8bc4\u4f30","text":""},{"location":"2.%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BD%95/#1pytorch","title":"1pytorch \u6709\u4e24\u79cd\u4e8c\u5143\u4ea4\u53c9\u71b5\u5b9e\u73b0\uff1a","text":"<ol> <li><code>torch.nn.BCELoss()</code>- \u521b\u5efa\u4e00\u4e2a\u635f\u5931\u51fd\u6570\uff0c\u7528\u4e8e\u6d4b\u91cf\u76ee\u6807\uff08\u6807\u7b7e\uff09\u548c\u8f93\u5165\uff08\u7279\u5f81\uff09\u4e4b\u95f4\u7684\u4e8c\u5143\u4ea4\u53c9\u71b5\u3002</li> <li><code>torch.nn.BCEWithLogitsLoss()</code>- \u8fd9\u4e0e\u4e0a\u9762\u76f8\u540c\uff0c\u53ea\u662f\u5b83\u6709\u4e00\u4e2a<code>nn.Sigmoid</code>\u5185\u7f6e\u7684 sigmoid \u5c42 ( )\uff08\u6211\u4eec\u5f88\u5feb\u5c31\u4f1a\u770b\u5230\u8fd9\u610f\u5473\u7740\u4ec0\u4e48\uff09\u3002</li> </ol> <pre><code># Create a loss function\n# loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in\nloss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid built-in\n\n# Create an optimizer\noptimizer = torch.optim.SGD(params=model_0.parameters(), \n                            lr=0.1)\n</code></pre>"},{"location":"2.%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BD%95/#2_2","title":"2.\u591a\u5143\u635f\u5931\u51fd\u6570","text":"<pre><code># Create loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model_4.parameters(), \n                            lr=0.1) # exercise: try changing the learning rate here and seeing what happens to the model's performance\n</code></pre>"},{"location":"2.%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BD%95/#2calculate-accuracy-a-classification-metric","title":"2.Calculate accuracy (a classification metric)","text":"<pre><code>def accuracy_fn(y_true, y_pred):\n    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n    acc = (correct / len(y_pred)) * 100 \n    return acc\n</code></pre>"},{"location":"2.%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BD%95/#3","title":"3.\u5206\u7c7b\u6307\u6807","text":"\u6307\u6807 \u63cf\u8ff0 \u94fe\u63a5 Accuracy Out of 100 predictions, how many does your model get correct? E.g. 95% accuracy means it gets 95/100 predictions correct. <code>torchmetrics.Accuracy()</code> or <code>sklearn.metrics.accuracy_score()</code> Precision Proportion of true positives over total number of samples. Higher precision leads to less false positives (model predicts 1 when it should've been 0). <code>torchmetrics.Precision()</code> or <code>sklearn.metrics.precision_score()</code> Recall Proportion of true positives over total number of true positives and false negatives (model predicts 0 when it should've been 1). Higher recall leads to less false negatives. <code>torchmetrics.Recall()</code> or <code>sklearn.metrics.recall_score()</code> F1-score Combines precision and recall into one metric. 1 is best, 0 is worst. <code>torchmetrics.F1Score()</code> or <code>sklearn.metrics.f1_score()</code> Confusion matrix Compares the predicted values with the true values in a tabular way, if 100% correct, all values in the matrix will be top left to bottom right (diagnol line). <code>torchmetrics.ConfusionMatrix</code> or <code>sklearn.metrics.plot_confusion_matrix()</code> Classification report Collection of some of the main classification metrics such as precision, recall and f1-score. <code>sklearn.metrics.classification_report()</code>Accuracy <p>\u793a\u8303</p> <pre><code>try:\n    from torchmetrics import Accuracy\nexcept:\n    !pip install torchmetrics==0.9.3 # this is the version we're using in this notebook (later versions exist here: https://torchmetrics.readthedocs.io/en/stable/generated/CHANGELOG.html#changelog)\n    from torchmetrics import Accuracy\n\n# Setup metric and make sure it's on the target device\ntorchmetrics_accuracy = Accuracy(task='multiclass', num_classes=4).to(device)\n\n# Calculate accuracy\ntorchmetrics_accuracy(y_preds, y_blob_test)\n</code></pre>"},{"location":"2.%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BD%95/#_3","title":"\u4e09\u3001\u6a21\u578b\u7684\u8f93\u51fa","text":""},{"location":"2.%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BD%95/#1logits","title":"1.logits","text":"<p>Logits \u89e3\u91ca\u4e3a\u6a21\u578b\u7684\u672a\u6807\u51c6\u5316\uff08\u6216\u5c1a\u672a\u6807\u51c6\u5316\uff09\u9884\u6d4b\uff08\u6216\u8f93\u51fa\uff09 \u3002*\u8fd9\u4e9b\u53ef\u4ee5\u7ed9\u51fa\u7ed3\u679c\uff0c\u4f46\u6211\u4eec\u901a\u5e38\u4e0d\u4f1a\u505c\u6b62\u4e8e logits\uff0c\u56e0\u4e3a\u89e3\u91ca\u5b83\u4eec\u7684\u539f\u59cb\u503c\u5e76\u4e0d\u5bb9\u6613\u3002\u67e5\u770b\u5b83\u4eec\u7684\u5b9a\u4e49\u4ee5\u5e2e\u52a9\u7406\u89e3logits\u662f\u5982\u4f55\u4ea7\u751f\u7684\u3002</p> <pre><code># \u67e5\u770b\u6d4b\u8bd5\u6570\u636e\u524d\u5411\u4f20\u64ad\u7684\u524d 5 \u4e2a\u8f93\u51fa \ny_logits = model_0(X_test.to(device))[:5] \ny_logits\n</code></pre>"},{"location":"2.%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BD%95/#2-logits","title":"2.\u5bf9 logits \u8fdb\u884c\u6807\u51c6\u5316","text":""},{"location":"2.%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BD%95/#1softmax","title":"\uff081\uff09softmax","text":"<p>\u4f7f\u7528softmax\uff0c\u5176\u4e2d\u6240\u6709\u7ed3\u679c\u603b\u548c\u4e3a<code>1</code>\uff0c\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u5c06\u5b83\u4eec\u89c6\u4e3a\u6982\u7387\uff1a</p> <pre><code># \u5bf9\u6a21\u578b logits \u4f7f\u7528 sigmoid \ny_pred_probs = torch.sigmoid(y_logits) \ny_pred_probs\n</code></pre> <p>\u5bf9 sigmoid \u6fc0\u6d3b\u51fd\u6570\u7684\u8f93\u51fa\u8fdb\u884c\u820d\u5165</p> <ul> <li>\u5982\u679c<code>y_pred_probs</code>&gt;= 0.5\uff0c<code>y=1</code>\uff081 \u7ea7\uff09</li> <li>\u5982\u679c<code>y_pred_probs</code>&lt; 0.5\uff0c<code>y=0</code>\uff080 \u7ea7\uff09</li> </ul> <pre><code># \u627e\u5230\u9884\u6d4b\u6807\u7b7e\uff08\u56f4\u7ed5\u9884\u6d4b\u6982\u7387\uff09\ny_preds  =  torch . round ( y_pred_probs ) \n</code></pre>"},{"location":"2.%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BD%95/#2argmax","title":"\uff082\uff09argmax \u591a\u5206\u7c7b","text":"<pre><code># \u5c06\u9884\u6d4b\u7684 logits \u8f6c\u6362\u4e3a\u9884\u6d4b\u6982\u7387\ny_pred_probs  =  torch . softmax ( y_logits ,  dim = 1 ) \n\n# \u5c06\u9884\u6d4b\u6982\u7387\u8f6c\u6362\u4e3a\u9884\u6d4b\u6807\u7b7e\ny_preds  =  y_pred_probs . argmax ( dim = 1 ) \n</code></pre>"},{"location":"2.%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BD%95/#3_1","title":"3.\u8bad\u7ec3\u6548\u679c","text":""},{"location":"2.%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E6%A8%A1%E5%9E%8B%E8%AE%B0%E5%BD%95/#1plot_decision_boundary","title":"\uff081\uff09plot_decision_boundary()","text":"<p>\u5b83\u5305\u542b\u4e00\u4e2a\u6709\u7528\u7684\u51fd\u6570\uff0c\u540d\u4e3a<code>plot_decision_boundary()</code>\uff0c\u5b83\u521b\u5efa\u4e00\u4e2a NumPy \u7f51\u683c\u6765\u76f4\u89c2\u5730\u7ed8\u5236\u6211\u4eec\u7684\u6a21\u578b\u9884\u6d4b\u67d0\u4e9b\u7c7b\u7684\u4e0d\u540c\u70b9\u3002</p> <pre><code>import requests\nfrom pathlib import Path \n\n# Download helper functions from Learn PyTorch repo (if not already downloaded)\nif Path(\"helper_functions.py\").is_file():\n  print(\"helper_functions.py already exists, skipping download\")\nelse:\n  print(\"Downloading helper_functions.py\")\n  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n  with open(\"helper_functions.py\", \"wb\") as f:\n    f.write(request.content)\n\nfrom helper_functions import plot_predictions, plot_decision_boundary\n</code></pre> <pre><code># Plot decision boundaries for training and test sets\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_0, X_train, y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_0, X_test, y_test)\n</code></pre> <p></p>"}]}